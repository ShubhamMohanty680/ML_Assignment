{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60733df",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9206a",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression (GBR) is a machine learning algorithm that belongs to the family of boosting algorithms. It is a supervised learning algorithm used for both regression and classification problems.\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning technique that combines weak regression models, typically decision trees, to make accurate predictions for regression tasks. It iteratively trains models to minimize the errors made by previous models, gradually improving predictions. By combining the strengths of multiple models, it captures complex relationships in the data and produces robust regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a452c9",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ff77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1216.38\n",
      "R-squared: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    \n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.intercept = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.intercept = np.mean(y)\n",
    "        residual = y - self.intercept\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            self.trees.append(tree)\n",
    "            pred = tree.predict(X)\n",
    "            residual -= self.learning_rate * pred\n",
    "            \n",
    "    def predict(self, X):\n",
    "        preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return self.intercept + self.learning_rate * np.sum(preds, axis=0)\n",
    "\n",
    "# Generate a random regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_train = int(0.8 * len(X))\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]\n",
    "\n",
    "# Train a gradient boosting regressor on the training set\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69314d17",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f8c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (Grid Search):  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
      "Best MSE (Grid Search):  876.6888496274581\n",
      "Best Hyperparameters (Random Search):  {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.1}\n",
      "Best MSE (Random Search):  1148.8147424966003\n",
      "MSE on Test Set (Best Model):  1189.2080757166354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dummy regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regression model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding MSE\n",
    "print(\"Best Hyperparameters (Grid Search): \", grid_search.best_params_)\n",
    "print(\"Best MSE (Grid Search): \", -grid_search.best_score_)\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(model, param_dist, cv=5, scoring='neg_mean_squared_error', n_iter=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding MSE\n",
    "print(\"Best Hyperparameters (Random Search): \", random_search.best_params_)\n",
    "print(\"Best MSE (Random Search): \", -random_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE on Test Set (Best Model): \", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718cff4a",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e97423",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a machine learning algorithm that performs only slightly better than random guessing, but is still able to learn from the training data. The concept of a weak learner in Gradient Boosting algorithm, combines many weak learners to create a strong learner.\n",
    "\n",
    "In Gradient Boosting, each weak learner is typically a decision tree with a shallow depth, often referred to as a decision stump. During training, the algorithm fits the weak learner to the data, and then adjusts the weights of the training examples to focus on the examples that were poorly predicted by the previous weak learner. The algorithm then adds the new weak learner to the ensemble, and repeats this process many times to gradually improve the model's accuracy.\n",
    "\n",
    "The strength of the Gradient Boosting algorithm lies in its ability to combine many weak learners to create a strong learner. By adding many weak learners and focusing on the examples that are difficult to predict, the algorithm is able to learn complex relationships in the data and create a highly accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816ed8bd",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf46350",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve the accuracy of a model by combining many weak learners to create a strong learner.\n",
    "\n",
    "The algorithm starts by fitting a weak learner to the training data. This initial model may not be very accurate, but it provides a starting point for the algorithm to build on. The algorithm then evaluates the errors made by this model, and focuses on the examples that were poorly predicted. The next weak learner is then trained to focus on these difficult examples, in an attempt to correct the errors made by the previous model.\n",
    "\n",
    "This process of adding a new weak learner and adjusting the weights of the examples is repeated many times, with each new weak learner improving on the errors made by the previous model. The final model is then created by combining all of the weak learners into a single ensemble model.\n",
    "\n",
    "The key idea behind Gradient Boosting is that each weak learner focuses on the examples that are difficult to predict, and by combining many of these weak learners, the algorithm is able to learn complex relationships in the data and create a highly accurate model. The \"gradient\" in the name comes from the fact that the algorithm uses the gradient of the loss function to determine how to adjust the weights of the training examples at each step, in order to focus on the examples that are difficult to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2ebce",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f5465",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new weak learners to the ensemble and adjusting the weights of the training examples to focus on the examples that are difficult to predict. This algorithm works as follows:\n",
    "\n",
    "**1. Initialize the model:** The algorithm starts by initializing the model with a single weak learner, such as a decision tree with a shallow depth.  \n",
    "**2. Fit the model:** The weak learner is fit to the training data, and the model makes predictions on the training set.  \n",
    "**3. Compute the residuals:** The algorithm computes the difference between the predicted values and the true values on the training set. These differences are called the residuals.  \n",
    "**4. Update the weights:** The weights of the training examples are adjusted based on the residuals. Examples, or simply features, that were poorly predicted by the previous model are given more weight, while examples that were well predicted are given less weight.  \n",
    "**5. Fit a new weak learner** A new weak learner is fit to the training data, with the weights of the examples, or simply features, that were adjusted to focus on the examples that are difficult to predict.  \n",
    "**6. Add the new weak learner to the ensemble:** The new weak learner is added to the ensemble of weak learners.  \n",
    "**7. Repeat:** Steps 2-6 are repeated many times, with each new weak learner focusing on the examples that are difficult to predict, and the weights of the training examples adjusted to emphasize these examples.  \n",
    "**8. Combine the weak learners:** The final model is created by combining all of the weak learners in the ensemble. The predictions of the weak learners are weighted according to their performance on the training set, and the weighted sum of the predictions is used to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce2c52",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29372bec",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm can be broken down into several key steps:\n",
    "\n",
    "1. Define the Loss Function: The first step is to define a loss function that measures the difference between the predicted values and the true values. Common loss functions for regression problems include the mean squared error (MSE) and the mean absolute error (MAE). For classification problems, common loss functions include the binary cross-entropy loss and the multi-class cross-entropy loss.\n",
    "2. Fit the Initial Model: The algorithm starts by fitting an initial model to the data, such as a decision tree with a shallow depth. This initial model may not be very accurate, but it provides a starting point for creating an algorithm.\n",
    "3. Compute the Residuals: The algorithm computes the difference between the predicted values and the true values on the training set. These differences are called the residuals.\n",
    "4. Fit a New Weak Learner: The next step is to fit a new weak learner to the residuals. The new weak learner should focus on the examples that were poorly predicted by the previous model, in an attempt to correct the errors made by the previous model. The most common choice for a weak learner in Gradient Boosting is a decision tree with a shallow depth.\n",
    "5. Update the Model: The predictions of the new weak learner are added to the predictions of the previous model, and the model is updated to minimize the loss function. This is typically done using a gradient descent algorithm, where the gradient of the loss function is used to determine the direction and magnitude of the update.\n",
    "6. Repeat: Steps 3-5 are repeated many times, with each new weak learner focusing on the examples that are difficult to predict, and the model updated to minimize the loss function.\n",
    "7. Combine the Weak Learners: The final model is created by combining all of the weak learners in the ensemble. The predictions of the weak learners are weighted according to their performance on the training set, and the weighted sum of the predictions is used to make the final prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
