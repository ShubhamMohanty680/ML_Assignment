{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2405021f",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b17b02",
   "metadata": {},
   "source": [
    "Lasso regression is a type of regression analysis that includes a penalty term i.e. the sum of absolute values of the model's coefficients.This encourages the model to reduce the magnitude of less important coefficients to zero, resulting in feature selection.\n",
    "\n",
    "Lasso differs from other regression techniques like Ridge regression, which penalizes the sum of squared coefficients, and does not perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57c52e",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e986e9",
   "metadata": {},
   "source": [
    "Lasso regression can perform feature selection by shrinking the coefficients of less important features to zero. This means that Lasso can identify the most important features for predicting the outcome variable and remove the less important ones, which can improve the model's performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1094e3",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f273b15",
   "metadata": {},
   "source": [
    "In Lasso regression, the coefficients represent the strength and direction of the relationship between the independent variables and the dependent variable. A positive coefficient indicates a positive relationship, while a negative coefficient indicates a negative relationship. The magnitude of the coefficient represents the strength of the relationship. The coefficients in Lasso regression can also be interpreted as feature importance or feature selection, where variables with non-zero coefficients are considered important predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc13b3",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc662ea",
   "metadata": {},
   "source": [
    "In Lasso Regression, there is a tuning parameter called alpha that controls the strength of regularization. A higher value of alpha results in a more restricted model with fewer features selected, while a lower value of alpha allows more features to be included in the model. Thus, the choice of alpha should balance between model complexity and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b0a44",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f077b0e",
   "metadata": {},
   "source": [
    "Yes, Lasso regression can be used for non-linear regressions.Lasso regression is primarily used for linear regression problems, but it can also be extended to non-linear regression problems by including non-linear transformations of the features. For example, polynomial regression can be combined with Lasso regularization to fit non-linear functions. However, this can increase the complexity of the model and the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f207eb1",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7f3047",
   "metadata": {},
   "source": [
    "Ridge and Lasso regression are two common techniques used in machine learning to reduce the impact of irrelevant or highly correlated features in a model. The difference is :\n",
    "\n",
    "\n",
    "Ridge regression shrinks the regression coefficients towards zero, while Lasso regression can shrink coefficients to exactly zero, effectively removing \n",
    "features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a502c31",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5096752f",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features by introducing a penalty term that shrinks the regression coefficients towards zero, effectively selecting only the most important features. This penalty term encourages the coefficients of correlated features to be close to each other or zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5486a4",
   "metadata": {},
   "source": [
    "# Q.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345fefe",
   "metadata": {},
   "source": [
    "To choose the optimal value of the regularization parameter lambda in Lasso Regression, one can use cross-validation to evaluate different values of lambda and select the one that gives the best balance between model complexity and accuracy. Essentially, you want to find the value of lambda that minimizes the error of the model while also preventing overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
