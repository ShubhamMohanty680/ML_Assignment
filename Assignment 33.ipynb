{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbfac0a",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d9849",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a type of supervised machine learning algorithm used for classification and regression tasks. In K-Nearest Neighbors (KNN), a data point is classified by finding the k-nearest neighbors in the training dataset, and assigning the class or value of the majority of these neighbors to the new data point.\n",
    "\n",
    "KNN is a simple and easy-to-implement algorithm, and it can work well on small to medium-sized datasets. However, it can become computationally expensive as the size of the dataset grows since it requires the calculation of the distances between the new data point and all the training instances. It is also sensitive to the choice of the value of K and the distance metric used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74292824",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8287109",
   "metadata": {},
   "source": [
    "Choosing the value of K in KNN is an important step in the algorithm as it can significantly impact the performance of the model. Here are some common methods for choosing the value of K:\n",
    "\n",
    "1. **Domain knowledge:** If you have knowledge of the domain or problem you are trying to solve, you may have an idea of what value of K would be appropriate. For example, if you are classifying images of handwritten digits, you might know that the digits are often written in a way that makes them appear more similar to their neighboring digits. In this case, a smaller value of K might be more appropriate.  \n",
    "2. **Cross-validation:** You can use cross-validation to evaluate the performance of the algorithm with different values of K. This involves dividing the data into training and validation sets and then evaluating the accuracy of the model on the validation set for different values of K. The value of K that gives the best performance on the validation set can be chosen through cross-validation.  \n",
    "3. **Grid search:** Another approach is to perform a grid search over a range of K values and evaluate the performance of the model for each value of K. The value of K that gives the best performance can be chosen.   \n",
    "\n",
    "It's important to keep in mind that the optimal value of K may depend on the specific dataset and problem you are working on, so it's a good idea to experiment with different values of K and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487d930",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa434",
   "metadata": {},
   "source": [
    "**KNN classifier** is used when the output variable is categorical or discrete, and the goal is to classify the new input data point into one of the predefined classes. For example, given a dataset of images of cats and dogs with their corresponding labels, a KNN classifier can be trained to classify a new image as either a cat or a dog.\n",
    "\n",
    "**KNN regressor** is used when the output variable is continuous, and the goal is to predict a numerical value. For example, given a dataset of house prices with their corresponding features (e.g., number of rooms, area, etc.), a KNN regressor can be trained to predict the price of a new house based on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef23c7",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa9d11",
   "metadata": {},
   "source": [
    "The performance of the KNN algorithm can be measured using various evaluation metrics, depending on whether it is used for classification or regression tasks. Here are some commonly used metrics:\n",
    "\n",
    "**For classification tasks:**\n",
    "\n",
    "1. **Accuracy:** This is the most common metric for classification tasks, and it measures the percentage of correctly classified instances over the total number of instances.\n",
    "\n",
    "2. **Precision and Recall:** Precision measures the percentage of correctly classified positive instances over the total number of instances classified as positive. Recall, on the other hand, measures the percentage of correctly classified positive instances over the total number of positive instances in the dataset.\n",
    "\n",
    "3. **F1 Score:** This is the harmonic mean of precision and recall and is often used as a more balanced measure that takes both precision and recall into account.\n",
    "\n",
    "4. **Confusion Matrix:** This is a table that shows the number of instances that were correctly or incorrectly classified by the algorithm for each class.\n",
    "\n",
    "**For regression tasks:**\n",
    "\n",
    "1. **Mean Squared Error (MSE):** This measures the average squared difference between the predicted and actual values.\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE):** This is the square root of the MSE and is often used as a more interpretable metric since it is in the same units as the output variable.\n",
    "\n",
    "3. **R-squared (R2):** This measures the proportion of variance in the output variable that can be explained by the algorithm.\n",
    "\n",
    "4. **Mean Absolute Error (MAE):** This measures the average absolute difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bad9c",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a42fad",
   "metadata": {},
   "source": [
    "The curse of dimensionality is a phenomenon that occurs in high-dimensional spaces, where the volume of the space increases exponentially with the number of dimensions. In KNN algorithm, the curse of dimensionality refers to the fact that as the number of features or dimensions in the dataset increases, the performance of the algorithm tends to deteriorate.\n",
    "\n",
    "This is because, as the number of dimensions increases, the distance between data points becomes less informative, and the nearest neighbors of a given point may not be truly representative of its local neighborhood. This is because, in high-dimensional spaces, most of the distance between points is concentrated around the edges or corners of the space, which makes it harder for KNN to accurately identify the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d5d69",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589851a",
   "metadata": {},
   "source": [
    "Handling missing values is an important step in any machine learning algorithm, including KNN. Here are some common strategies for handling missing values in KNN:\n",
    "\n",
    "1. **Deletion:** One approach is to simply remove any data points that have missing values. This can be effective if the amount of missing data is small and does not significantly impact the size of the dataset.\n",
    "2. **Imputation:** Another approach is to replace missing values with estimated values. Common imputation methods include mean imputation, median imputation, and mode imputation. In mean imputation, the missing values are replaced with the mean value of the available data points. Similarly, in median imputation, the missing values are replaced with the median value of the available data points. In mode imputation, the missing values are replaced with the mode (most common) value of the available data points.\n",
    "3. **KNN imputation:** This is a specific approach that uses KNN algorithm to predict missing values based on the values of the K-nearest neighbors. In this approach, the missing values are replaced with the mean or median value of the K-nearest neighbors in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7341f40",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d996f4d",
   "metadata": {},
   "source": [
    "The KNN classifier and regressor have different performance metrics and are suitable for different types of problems.\n",
    "\n",
    "**KNN Classifier:** The KNN classifier is a supervised learning algorithm used for classification problems. The algorithm predicts the class of an instance based on the class labels of its nearest neighbors. The performance of the KNN classifier can be evaluated using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**KNN Regressor:** The KNN regressor is also a supervised learning algorithm used for regression problems. The algorithm predicts the value of an instance based on the values of its nearest neighbors. The performance of the KNN regressor can be evaluated using metrics such as mean squared error (MSE), root mean squared error (RMSE), and R-squared.\n",
    "\n",
    "Comparing the two algorithms, the KNN classifier is better suited for problems where the output variable is categorical or discrete, while the KNN regressor is better suited for problems where the output variable is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec106dc",
   "metadata": {},
   "source": [
    "# Q.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdc2c5",
   "metadata": {},
   "source": [
    "**Strengths of KNN algorithm:**\n",
    "\n",
    "1. Simple and easy to implement.\n",
    "2. Non-parametric, meaning it does not assume any specific form of the underlying data distribution.\n",
    "3. Effective for multi-class classification problems.\n",
    "4. Can handle non-linear and complex decision boundaries.\n",
    "5. Can be used for both classification and regression tasks.\n",
    "\n",
    "**Weaknesses of KNN algorithm:**\n",
    "\n",
    "1. Computationally expensive, as the distance between the new data point and all the training data points needs to be calculated.\n",
    "2. Sensitive to the choice of distance metric and the value of K.\n",
    "3. Performance degrades with increasing dimensionality of the data.\n",
    "4. Limited generalization ability, as it memorizes the training data rather than learning a model from the data.\n",
    "5. Can be affected by imbalanced datasets.\n",
    "\n",
    "**Here are some ways to address the weaknesses of the KNN algorithm:**\n",
    "\n",
    "1. Use dimensionality reduction techniques such as PCA or t-SNE to reduce the number of features.\n",
    "2. Use distance metrics that are more appropriate for the data, such as Manhattan or Minkowski distances.\n",
    "3. Use techniques such as cross-validation to show the optimal value of K.\n",
    "4. Implement algorithms that reduce the number of distance calculations, such as the KD-tree or ball tree algorithms.\n",
    "5. Use ensemble methods such as bagging or boosting to improve generalization performance.\n",
    "6. Address imbalanced datasets by oversampling or undersampling techniques, or by using weighted KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f0e5d",
   "metadata": {},
   "source": [
    "# Q.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009525fb",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm. The main difference between them is how they calculate the distance between two points.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squares of the differences between the corresponding coordinates of the two points. Mathematically, the Euclidean distance between two points (x1, y1) and (x2, y2) can be expressed as:\n",
    "\n",
    "    d = âˆš((x2 - x1)^2 + (y2 - y1)^2)\n",
    "\n",
    "Manhattan distance, also known as taxicab distance or L1 norm, is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between the corresponding coordinates of the two points. Mathematically, the Manhattan distance between two points (x1, y1) and (x2, y2) can be expressed as:\n",
    "\n",
    "    d = |x2 - x1| + |y2 - y1|\n",
    "\n",
    "The main difference between Euclidean and Manhattan distance is that Euclidean distance takes into account the magnitude and direction of the differences between the coordinates, while Manhattan distance only takes into account the magnitude of the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be908941",
   "metadata": {},
   "source": [
    "# Q.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a6add",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step in KNN algorithm that helps to improve the performance and accuracy of the algorithm. The reason for this is that KNN is a distance-based algorithm, which means that it relies on the distance between data points to make predictions. If the features have different scales, the distance between the data points will be dominated by the features with the largest scale, leading to biased predictions.\n",
    "\n",
    "By scaling the features, the KNN algorithm can give equal importance to all the features, leading to more accurate and unbiased predictions. Additionally, feature scaling can also help to reduce the computational cost of the algorithm, as it reduces the dimensionality of the feature space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
