{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007ee249",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2f6f39",
   "metadata": {},
   "source": [
    "Linear regression is a statistical method used to establish a relationship between a dependent variable and one or more independent variables. It predicts a continuous output variable based on a linear combination of input variables.\n",
    "\n",
    "\n",
    "Logistic regression, on the other hand, is used to model the probability of a binary or categorical outcome based on one or more predictor variables. It is used when the response variable is categorical.\n",
    "\n",
    "\n",
    "An example scenario where logistic regression would be more appropriate is when analyzing the factors that contribute to the likelihood of a person purchasing a product.The outcome of the purchase decision is binary (either the person purchased the product or not), making logistic regression the appropriate modeling approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51d0fa",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d53c4e",
   "metadata": {},
   "source": [
    "The cost function used in logistic regression is called cross-entropy loss function. It measures the difference between predicted probabilities and actual target values. The optimization of the cost function is performed using gradient descent, which updates the model parameters in the direction of the steepest descent of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f68c43",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50301ab",
   "metadata": {},
   "source": [
    "Regularization is a technique used in logistic regression to prevent overfitting by adding a penalty term to the cost function. This penalty term discourages the model from assigning high weights to input features, thereby reducing their impact on the final output. Regularization helps to improve the model's generalization performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06fa5e",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d172ce",
   "metadata": {},
   "source": [
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier, such as a logistic regression model, at different classification thresholds. It plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. \n",
    "         \n",
    "         \n",
    "The area under the ROC curve (AUC) is a metric used to evaluate the performance of the model, where an AUC of 1.0 represents a perfect classifier and an AUC of 0.5 represents a random classifier. A higher AUC indicates better model performance in distinguishing between positive and negative classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30c0b4",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eefc935",
   "metadata": {},
   "source": [
    "Common techniques for feature selection in logistic regression include backward elimination, forward selection, and Lasso regularization. \n",
    "These techniques help improve the model's performance by selecting the most relevant features and reducing the impact of irrelevant or redundant features, which can lead to overfitting and decreased model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540bbcd",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d9b2",
   "metadata": {},
   "source": [
    "Imbalanced datasets in logistic regression can be handled using techniques such as oversampling the minority class, undersampling the majority class, or using a combination of both. Other strategies include changing the decision threshold, using cost-sensitive learning, and using ensemble methods such as bagging or boosting. These techniques help to improve the model's performance on the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7f100",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a47ed",
   "metadata": {},
   "source": [
    "In logistic regression, multicollinearity, overfitting, class imbalance, and outliers are some of the common issues and challenges that can arise. To address multicollinearity, one can perform feature selection or use regularization techniques like Lasso or Ridge regression. To address overfitting, regularization methods like Ridge or Lasso regression can be employed. Class imbalance can be handled using techniques such as oversampling, undersampling, or using a combination of both. Finally, outliers can be detected and removed using appropriate techniques such as the Z-score or IQR methods, or robust regression techniques like Huber regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
