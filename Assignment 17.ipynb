{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f66431",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52a80e",
   "metadata": {},
   "source": [
    "Grid search CV (Cross-Validation) is a hyperparameter tuning technique used to find the best combination of hyperparameters for a machine learning model. It works by exhaustively searching through a predefined set of hyperparameters and evaluating the model's performance on each combination using cross-validation. The combination of hyperparameters that produces the best performance is selected as the optimal set of hyperparameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c85689",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e711a",
   "metadata": {},
   "source": [
    "Grid search CV and random search CV are both hyperparameter tuning techniques in machine learning, but differ in the way they explore the hyperparameter space. Grid search exhaustively searches through all possible hyperparameter combinations, while random search randomly samples from the hyperparameter space. Random search is faster and more effective for high-dimensional hyperparameter spaces, while grid search is more suitable for small hyperparameter spaces or when the relative importance of each hyperparameter is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad94f5",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01021f9",
   "metadata": {},
   "source": [
    "Data leakage occurs when information from the test set is unintentionally used to influence the training of the model, leading to overly optimistic performance estimates and poor generalization to new data. It occurs when there is a flow of information from the target variable or the evaluation dataset into the training data, providing the model with information it would not have access to in a real-world scenario.\n",
    "\n",
    "\n",
    "An example of data leakage is in credit card fraud detection, where the dataset includes future timestamps of transactions. The model inadvertently learns from this future information and becomes overly accurate in detecting fraud during training. However, when deployed in the real world, the model fails to perform well because it doesn't have access to future information. This leakage of future timestamps into the training process leads to an inaccurate and unreliable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7223f",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29583019",
   "metadata": {},
   "source": [
    "To prevent data leakage in machine learning, it's important to keep the training and testing datasets separate or isolated and ensure that the model is not exposed to any information in the testing set during training. Additionally, feature selection, data preprocessing, and hyperparameter tuning should be performed using only the training set and cross-validation, rather than the entire dataset, to prevent overfitting and ensure unbiased model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30489d60",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc353ac",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted and actual class labels of a set of data. It shows the number of true positives, false positives, true negatives, and false negatives, allowing for the calculation of metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8572dc",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23860969",
   "metadata": {},
   "source": [
    "Precision and recall are metrics that help evaluate the performance of a classification model. Precision is the proportion of predicted positive instances that are actually positive, while recall is the proportion of actual positive instances that are correctly predicted as positive. In simpler terms, precision is the model's ability to correctly identify the positive cases among all predicted positive cases, while recall is the model's ability to identify all actual positive cases. A high precision means that the model makes few false positive predictions, while a high recall means that the model detects most of the actual positive cases.\n",
    "\n",
    "\n",
    "The formula is as:\n",
    "1. Precision=TP/(TP+FP)\n",
    "2. Recall=TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d256725",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef503f57",
   "metadata": {},
   "source": [
    "To interpret a confusion matrix and determine which types of errors a model is making, one can examine the false positives and false negatives. False positives represent cases where the model predicted a positive class label when the actual label is negative, while false negatives represent cases where the model predicted a negative class label when the actual label is positive. By examining these errors, one can identify areas of the model that require improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc24f771",
   "metadata": {},
   "source": [
    "# Q.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575ed76",
   "metadata": {},
   "source": [
    "Common metrics that can be derived from a confusion matrix include accuracy, precision, recall, F1-score, and the area under the ROC curve. \n",
    "\n",
    "\n",
    "Accuracy is calculated as (TP+TN)/(TP+TN+FP+FN), precision as TP/(TP+FP), recall as TP/(TP+FN), F1-score as 2 * ((precision * recall)/(precision+recall)), and the area under the ROC curve as a measure of the model's ability to discriminate between positive and negative classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca6618",
   "metadata": {},
   "source": [
    "# Q.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0494e5",
   "metadata": {},
   "source": [
    "The accuracy of a model is calculated from the values in its confusion matrix and represents the proportion of correctly classified instances over the total number of  instances. Specifically, accuracy is calculated as (TP+TN)/(TP+TN+FP+FN), where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives. The accuracy metric alone, however, may not provide a complete picture of a model's performance, especially in the presence of imbalanced datasets or asymmetric costs of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e62b2",
   "metadata": {},
   "source": [
    "# Q.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eca196",
   "metadata": {},
   "source": [
    "A confusion matrix can help identify potential biases or limitations in a machine learning model by examining its distribution of predictions across different classes. For instance, if the model consistently misclassifies one particular class, it could indicate a bias or limitation in the model's ability to capture that class's features or patterns. Additionally, if the data is imbalanced and the model is biased towards the majority class, the confusion matrix can reveal the extent of this bias and prompt the use of techniques such as resampling or adjusting class weights to mitigate it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
