{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b8dff0",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55863b",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variable(s) in a linear regression model. It ranges from 0 to 1, with 1 indicating a perfect fit of the model to the data. It is calculated by dividing the explained variance by the total variance of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21b820",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221da1b2",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables used in a regression model. It addresses a potential limitation of R-squared by penalizing the addition of unnecessary variables to the model, thus providing a more accurate assessment of the model's explanatory power.\n",
    "\n",
    "Unlike regular R-squared, adjusted R-squared penalizes the addition of unnecessary independent variables, making it a better measure of a model's goodness of fit. This penalty adjusts for the possibility that adding more variables to the model might artificially inflate the regular R-squared, leading to an overfitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299cbb2",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d210230",
   "metadata": {},
   "source": [
    "- Adjusted R-squared is more appropriate when comparing multiple regression models with different numbers of independent variables. \n",
    "\n",
    "\n",
    "- It adjusts for the number of variables in the model, penalizing models with too many variables that do not contribute significantly to the model's overall fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a6009",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c44c6d",
   "metadata": {},
   "source": [
    "- RMSE, MSE, and MAE are metrics used to evaluate the performance of regression models.\n",
    "\n",
    "\n",
    "- RMSE represents the root mean squared error, MSE represents the mean squared error, and MAE represents the mean absolute error.\n",
    "\n",
    "\n",
    "- These metrics measure the difference between the predicted and actual values of the target variable, with RMSE and MSE giving more weight to larger errors, whereas MAE gives less weight to the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c36aac",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e360d4b0",
   "metadata": {},
   "source": [
    "RMSE and MSE take into account the magnitude of errors, giving more weight to larger errors. This means that these metrics can be more sensitive to outliers, which are data points that are significantly different from other data points. On the other hand, MAE treats all errors equally and is more robust to outliers.\n",
    "\n",
    "One disadvantage of RMSE, MSE, and MAE is that they do not provide information about the direction of errors, i.e., whether the model is overestimating or underestimating the target variable. Another disadvantage is that they do not take into account the relative importance of different types of errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec70f22",
   "metadata": {},
   "source": [
    "# Q.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea2855",
   "metadata": {},
   "source": [
    "Lasso regularization is a technique used to prevent overfitting in linear regression models by adding a penalty term to the cost function. \n",
    "\n",
    "\n",
    "It differs from Ridge regularization in a way that it shrinks some of the model coefficients to zero, effectively performing feature selection. \n",
    "\n",
    "\n",
    "Lasso regularization is more appropriate when the number of features is large and only a few are expected to be important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabfb2f",
   "metadata": {},
   "source": [
    "# Q.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7631ac8",
   "metadata": {},
   "source": [
    "Regularized linear models are a type of machine learning algorithm that can help prevent overfitting. They work by adding a penalty term to the loss function, which discourages the model from overemphasizing any one feature or parameter. This encourages the model to generalize better to new data, rather than simply memorizing the training data. \n",
    "\n",
    "For example, Lasso regression is a type of regularized linear model that can be used to select important features and reduce overfitting in a dataset by shrinking the coefficients of less important features towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718a592",
   "metadata": {},
   "source": [
    "# Q.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d4a85c",
   "metadata": {},
   "source": [
    "The limitation of regularized linear models is that they assume a linear relationship between the dependent and independent variables, which may not be true in real-world scenarios. \n",
    "\n",
    "\n",
    "Additionally, regularized linear models require tuning of the regularization parameter, which can be time-consuming and requires expertise. As a result, other non-linear regression models may be better suited for more complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31354d6",
   "metadata": {},
   "source": [
    "# Q.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee120bc5",
   "metadata": {},
   "source": [
    "The choice of the better model depends on the specific context and goals. If the goal is to minimize the overall magnitude of errors, choose Model A, as it has a lower RMSE. But if the goal is to minimize the average magnitude of errors, Model B, with the lower MAE, may be preferred.\n",
    "\n",
    "In short, the choice of the better-performing model between Model A and Model B depends on the specific context and the importance placed on different types of errors. Model A with a lower RMSE (Root Mean Squared Error) may be preferred if larger errors and outliers are of greater concern. On the other hand, Model B with a lower MAE (Mean Absolute Error) may be more appropriate if all errors are considered equally important or if the error distribution is expected to be skewed. \n",
    "\n",
    "It's important to consider multiple evaluation metrics and analyze other factors to make a comprehensive assessment of the models' performance.\n",
    "However, both metrics have limitations as they treat all errors equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a11a0",
   "metadata": {},
   "source": [
    "# Q.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c272b0",
   "metadata": {},
   "source": [
    "The choice of the better performing model depends on the specific context and goals of the analysis. Ridge regularization (Model A) is better suited for situations where there are many variables with small effects. Lasso regularization (Model B) is better suited for situations where there are only a few variables with large effects.\n",
    "\n",
    "However, Lasso may perform feature selection, which may be an advantage or disadvantage depending on the situation. The choice of regularization method should be based on the specific goals and characteristics of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
