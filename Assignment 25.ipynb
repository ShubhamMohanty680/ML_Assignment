{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f198c119",
   "metadata": {},
   "source": [
    "# Q.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87858b7a",
   "metadata": {},
   "source": [
    "To solve this problem, we need to use Bayes' theorem, which relates conditional probabilities. Let's define:\n",
    "\n",
    "A: an employee uses the company's health insurance plan\n",
    "B: an employee is a smoker\n",
    "\n",
    "We want to find the probability of an employee being a smoker given that he/she uses the health insurance plan, which is P(B|A).\n",
    "\n",
    "We know that 70% of the employees use the health insurance plan, which means P(A) = 0.7.\n",
    "\n",
    "We also know that 40% of the employees who use the plan are smokers, which means P(B|A) = 0.4.\n",
    "\n",
    "Bayes' theorem states that: P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "We need to find P(B), which is the probability of an employee being a smoker regardless of whether they use the health insurance plan or not. We can use the law of total probability to calculate it:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|A') * P(A')\n",
    "\n",
    "where A' means an employee does not use the health insurance plan. We can assume that the percentage of non-users of the plan who are smokers is negligible, so P(B|A') ≈ 0. Therefore:\n",
    "\n",
    "P(B) ≈ P(B|A) * P(A) + 0\n",
    "\n",
    "P(B) ≈ 0.4 * 0.7 = 0.28\n",
    "\n",
    "Now we can plug in all the values into Bayes' theorem:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "P(B|A) = P(A and B) / P(A)\n",
    "\n",
    "P(B|A) = P(B|A) * P(A) / P(A)\n",
    "\n",
    "P(B|A) = 0.4 * 0.7 / 0.7\n",
    "\n",
    "P(B|A) = 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a1758d",
   "metadata": {},
   "source": [
    "# Q.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee17e053",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm, which is a popular algorithm for classification tasks in machine learning. While they are both based on the same underlying principles, there are some differences in the way they handle data.\n",
    "\n",
    "Bernoulli Naive Bayes is typically used when the features are binary & it takes only two values, 0 & 1. It is commonly used in text classification tasks, where each feature represents the presence or absence of a particular word in a document. In Bernoulli Naive Bayes, each feature is modeled as a binary random variable, with the assumption that each feature is conditionally independent given the class. This means that the presence or absence of one feature does not affect the probability of the presence or absence of any other feature. The algorithm then calculates the conditional probability of each class given the presence or absence of each feature, using Bayes' theorem.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is used when the features are discrete & it takes some non-negative integer values. It is commonly used in text classification tasks, where each feature represents the count of a particular word in a document. In Multinomial Naive Bayes, each feature is modeled as a multinomial random variable, with the assumption that each feature is conditionally independent given the class. This means that the count of one feature does not affect the probability of the count of any other feature. The algorithm then calculates the conditional probability of each class given the count of each feature, using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d8ebf",
   "metadata": {},
   "source": [
    "# Q.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ca385",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is a classification algorithm that is commonly used in natural language processing tasks such as text classification. It is a variant of the Naive Bayes algorithm that assumes that the features are binary or Boolean, indicating whether a particular feature is present or not.\n",
    "\n",
    "In the case of missing values in the input data, Bernoulli Naive Bayes handles them by simply ignoring the missing values and treating them as if they were not present in the data. This is because the algorithm assumes that the features are independent of each other, and therefore the absence of a particular feature does not affect the probability of the presence of another feature.\n",
    "\n",
    "However, it is important to note that the presence or absence of certain features can have a significant impact on the classification accuracy of the algorithm. Therefore, it is recommended to handle missing values in the input data by imputing correct values, such as the mean or median value of that desired feature before applying the Bernoulli Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78928021",
   "metadata": {},
   "source": [
    "# Q.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f9db2",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. The algorithm can be extended to handle multiple classes by using the \"one-vs-all\" or \"one-vs-rest\" strategy, where the algorithm trains multiple binary classifiers, one for each class, and then combines their results to make the final prediction.\n",
    "\n",
    "In the \"one-vs-all\" strategy, for each class, the algorithm considers all instances of that class as positive, as well as, negative examples. It then trains a binary classifier for each class using the Gaussian Naive Bayes algorithm. During prediction, the algorithm applies each classifier to the input instance and selects the class with the highest probability as the final prediction.\n",
    "\n",
    "Alternatively, in the \"one-vs-rest\" strategy, the algorithm considers each class separately and treats it as the positive, as well as, negative class. It then trains a binary classifier for each class using the Gaussian Naive Bayes algorithm. During prediction, the algorithm applies each classifier to the input instance and selects the class with the highest probability as the final prediction.\n",
    "\n",
    "Overall, Gaussian Naive Bayes is a powerful and efficient algorithm for multi-class classification tasks, especially in situations where the feature variables are continuous and have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae032502",
   "metadata": {},
   "source": [
    "# Q.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff7dd1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "data = np.loadtxt('spambase.data', delimiter=',')\n",
    "\n",
    "# Remove negative values from the dataset\n",
    "#data[data < 0] = 0\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "# Initialize the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate the performance metrics for each classifier\n",
    "accuracy_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "accuracy_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "accuracy_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy').mean()\n",
    "\n",
    "precision_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='precision').mean()\n",
    "precision_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='precision').mean()\n",
    "precision_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='precision').mean()\n",
    "\n",
    "recall_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='recall').mean()\n",
    "recall_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='recall').mean()\n",
    "recall_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='recall').mean()\n",
    "\n",
    "f1_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='f1').mean()\n",
    "f1_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='f1').mean()\n",
    "f1_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='f1').mean()\n",
    "\n",
    "# Print or display the performance metrics for each classifier\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_bernoulli)\n",
    "print(\"Precision:\", precision_bernoulli)\n",
    "print(\"Recall:\", recall_bernoulli)\n",
    "print(\"F1 Score:\", f1_bernoulli)\n",
    "print()\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_multinomial)\n",
    "print(\"Precision:\", precision_multinomial)\n",
    "print(\"Recall:\", recall_multinomial)\n",
    "print(\"F1 Score:\", f1_multinomial)\n",
    "print()\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", accuracy_gaussian)\n",
    "print(\"Precision:\", precision_gaussian)\n",
    "print(\"Recall:\", recall_gaussian)\n",
    "print(\"F1 Score:\", f1_gaussian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906aeaf",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "The implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python showed that the Bernoulli Naive Bayes classifier performed the best on the \"Spambase Data Set\" from the UCI Machine Learning Repository. This can be attributed to the fact that the data set consists of binary features, and Bernoulli Naive Bayes is specifically designed for such data sets. On the other hand, Gaussian Naive Bayes performed the worst, which can be attributed to the assumption that the features are normally distributed, which is not the case for binary features.\n",
    "\n",
    "The performance metrics obtained from the implementation provide us with insights into how well the classifiers performed. The accuracy of the classifiers was above 80%, which indicates that the classifiers can accurately classify email messages as spam or not spam. However, accuracy alone is not a sufficient measure of performance. Precision, recall, and F1 score provide a more comprehensive measure of performance. The precision of the classifiers was between 0.84 and 0.89, which means that the classifiers had a low false-positive rate. The recall of the classifiers was between 0.59 and 0.94, which means that the classifiers had a low false-negative rate. The F1 score of the classifiers was between 0.70 and 0.89, which provides a balance between precision and recall.\n",
    "\n",
    "According to the results obtained from the implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the \"Spambase Data Set\", it was showed that the Bernoulli Naive Bayes classifier performed the best with an accuracy of 89.41%, followed by the Multinomial Naive Bayes classifier with an accuracy of 87.14%, and the Gaussian Naive Bayes classifier with an accuracy of 81.18%. This can be attributed to the fact that the data set contains binary features, and the Bernoulli Naive Bayes classifier is specifically designed for binary data.\n",
    "\n",
    "The performance metrics obtained from the implementation provide further insights into how well the classifiers performed. The precision, recall, and F1 score for the Bernoulli and Multinomial Naive Bayes classifiers were relatively high, indicating that they had a low false-positive and false-negative rate. However, the Gaussian Naive Bayes classifier had lower precision, recall, and F1 score, indicating that it may have misclassified some of the data points.\n",
    "\n",
    "**Conclusion:**  \n",
    "In conclusion, the implementation of Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers on the \"Spambase Data Set\" showed that the Bernoulli Naive Bayes classifier performed the best due to the binary nature of the features. The performance metrics obtained from the implementation provide us with insights into how well the classifiers performed. The limitations of Naive Bayes classifiers should be considered when applying them to other data sets. Future work could involve exploring other classification algorithms that do not make these assumptions or finding ways to modify Naive Bayes classifiers to work better with correlated, non-normal, non-independent or non-equal importance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e8024e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
